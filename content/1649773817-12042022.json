{"title":"Robotáxi foge da polícia / Primeiro computador analógico / Jeff Bezos no Brasil","fps":30,"timestamp":1649773817,"date":"12/04/2022","intro":{"text":"Olá pessoal, A seguir vocês acompanharão as notícias desta terça-feira."},"end":{"text":"Estas notícias foram extraídas da newsletter de Filipe Deschamps. Para acompanhar estas notícias em formato de texto, inscreva-se no link na descrição.","url":"https://links.codestack.me/newsletter-filipe"},"news":[{"text":"Robotáxi aparenta fugir durante abordagem policial nos EUA: o veículo autônomo, em fase de treinamento pela Cruise, uma parceria entre GM, Honda e Microsoft, foi parado por policiais em São Francisco por causa dos faróis desligados enquanto dirigia à noite. No entanto, durante a abordagem, o veículo volta a dirigir sozinho, forçando uma curta perseguição policial até o veículo parar novamente. A Cruise afirma que o veículo não estava tentando fugir, mas apenas tentando estacionar em um local mais seguro. O veículo não foi multado. As informações são do site Ars Technica.","url":""},{"text":"Pesquisa chega em provável “dia zero” da “Máquina de Anticítera”: o computador analógico mais antigo que se conhece teria uma data inicial para calibração de seu mecanismo – 23 de dezembro de 178 a.C., próximo da data do solstício de inverno no hemisfério norte. Saber o “dia zero” é fundamental para garantir a precisão do dispositivo. A Máquina de Anticítera, criada na Grécia, era utilizada para prever posições astronômicas e eclipses, como função de calendário e astrologia. As informações são do site Ars Technica.","url":""},{"text":"Jeff Bezos, da Amazon, realiza primeiro investimento no Brasil: o aporte de 45 milhões de dólares, por meio da “Bezos Expeditions”, sua empresa de investimentos pessoais, foi realizado na fintech brasileira “Stark Bank”, que oferece soluções financeiras para empresas, como cartão corporativo, integrações com ERP e APIs para desenvolvedores. As informações são do Tecnoblog.","url":""},{"text":"Conselho Nacional de Trânsito aprova multas através de câmeras de monitoramento online: agentes de trânsito poderão autuar condutores e veículos cometendo ou que cometeram infrações filmadas através de câmeras de videomonitoramento, 24 horas por dia. A fiscalização remota, no entanto, somente poderá ser realizada em vias devidamente sinalizadas. As informações são do site Canaltech.","url":""},{"text":"Startup israelense cria 100 retratos de “Mona Lisa” iguais para humanos, mas diferentes para IAs: as imagens da pintura famosa foram modificados pela “Adversa AI” para apenas confundirem sistemas de reconhecimento facial – cada retrato contém artefatos representando uma celebridade ou artista diferente. A técnica utilizada, “ataque adversário”, são como ilusões de óptica para máquinas e pode ser utilizada intencionalmente por cibercriminosos para hackear sistemas de reconhecimento facial, carros autônomos, imagens médicas, algoritmos financeiros ou qualquer outra tecnologia de IA. As informações são do site PR Newswire.","url":""},{"text":"Startup quer mapear fundo do mar com drones submarinos: a “Terradepth” almeja ser o “Google Earth” dos oceanos. Os robôs da companhia são completamente autônomos, com sistema de recarregamento de baterias e inteligência artificial embarcada. Os dados coletados serão valiosos para governos, empresas e organizações de pesquisa. Apenas 20% do fundo do mar já está mapeado, com a porção restante cobrindo o dobro da superfície de Marte. As informações são do site TechCrunch.","url":""},{"text":"Astronautas tentarão criar “lente líquida” para telescópios no espaço: durante a primeira missão espacial privada “Ax-1”, ocorrerá o “FLUTE” (Experimento Telescópio Fluídico, na sigla em inglês), método para produzir espelhos através de polímeros líquidos. A técnica poderia permitir à NASA construir observatórios com espelhos gigantes diretamente no espaço, sem precisar lançar esses componentes da Terra. As informações são do site Space.","url":""},{"text":"Novo modelo de linguagem da DeepMind supera outros modelos maiores: o “Chinchilla” possui “apenas” 70 bilhões de parâmetros, mas demonstra um resultado “consistentemente e significativamente melhor” do que o GPT-3 da OpenAI (175 bilhões de parâmetros), o “Gopher” (280 bi) da própria da DeepMind e o “Megatron-Turing NLG” (580 bi), maior modelo do mundo, desenvolvido pela Microsoft. Para o Chinchilla, os pesquisadores utilizaram uma abordagem diferente, otimizando não só o tamanho da base de dados, mas a quantidade de tokens de treinamento, demonstrando que os outros modelos estariam sub-treinados. As informações são do site MarkTechPost.","url":""}]}